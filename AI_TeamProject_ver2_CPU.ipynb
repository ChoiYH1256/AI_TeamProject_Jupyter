{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQaogG5gAt_j"
      },
      "source": [
        "\n",
        "## 목차\n",
        "1. 환경 설정 및 필요한 라이브러리 불러오기\n",
        "- PyTorch, torchvision 등 필요한 라이브러리 import\n",
        "- CUDA 사용 가능 여부 확인 및 device 설정\n",
        "\n",
        "2. COCO 데이터셋 준비\n",
        "- COCO 데이터셋 다운로드 및 경로 설정\n",
        "- COCO API 설치 및 사용 방법 소개\n",
        "- 데이터셋 클래스 정의 (CocoDetection 활용)\n",
        "- DataLoader 설정\n",
        "\n",
        "3. RetinaNet-ResNeXt50 모델 구현\n",
        "- ResNeXt50 백본 네트워크 정의\n",
        "- FPN (Feature Pyramid Network) 구현\n",
        "- RetinaNet 헤드 (분류 및 박스 회귀) 구현\n",
        "- Anchor 생성기 구현\n",
        "- 전체 RetinaNet-ResNeXt50 모델 조립\n",
        "\n",
        "4. 손실 함수 정의\n",
        "- Focal Loss 구현\n",
        "- Smooth L1 Loss 구현\n",
        "- 전체 손실 함수 조합\n",
        "\n",
        "5. 학습 루프 구현\n",
        "- 옵티마이저 설정 (예: Adam)\n",
        "- 학습률 스케줄러 설정\n",
        "- 에폭 단위 학습 함수 구현\n",
        "- 검증 함수 구현\n",
        "\n",
        "6. 모델 학습 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9k-b3vTEv8y"
      },
      "source": [
        "## CPU 모델 및 GPU 모델 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK1GhaRqErdj",
        "outputId": "168df9b2-1927-42ee-9b52-f6b844d8968a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#현재 CPU모델과 GPU모델 확인\n",
        "!cat /proc/cpuinfo | grep \"model name\" | uniq\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## import 및 패키지 다운"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!sudo apt install python3-pip\n",
        "\n",
        "!pip install --upgrade torchvision\n",
        "!pip install torch torchvision pycocotools pillow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.ops import box_iou, clip_boxes_to_image\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "from torchvision.models.detection.retinanet import RetinaNet, RetinaNetHead\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlfYFBVuaNTq"
      },
      "source": [
        "## 모델 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy9gOiwSZ_Zc"
      },
      "outputs": [],
      "source": [
        "config = {'print_inter' : 5,\n",
        "          'batch_size'  : 4,\n",
        "          'worker'      : 2,\n",
        "          'epochs'      : 1,\n",
        "          'momentum'    : 0.9,\n",
        "          'lr_decay'    : 0.0005,\n",
        "          'SGD_lr'      : 0.01,\n",
        "          'Adam_lr'     : 0.001,\n",
        "          }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "047qrCNBjF2r"
      },
      "source": [
        "## COCO 데이터셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgEIlbBeyegp",
        "outputId": "8efef7be-595f-4225-c48d-9263876c9a87"
      },
      "outputs": [],
      "source": [
        "#Colab에서 사용시 필요 (로컬시 주석처리)\n",
        "\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "fCUp2xUt0Y1B",
        "outputId": "fa0dc994-6687-4fae-c942-f44d7a084802"
      },
      "outputs": [],
      "source": [
        "# coco 데이터셋 다운로드 (구글드라이브) (colab에서 사용시 사용)\n",
        "# 필요시 주석 제거\n",
        "\n",
        "'''\n",
        "!wget -nc http://images.cocodataset.org/zips/train2017.zip -P /content/drive/MyDrive/\n",
        "!wget -nc http://images.cocodataset.org/zips/test2017.zip -P /content/drive/MyDrive/\n",
        "!wget -nc http://images.cocodataset.org/zips/val2017.zip -P /content/drive/MyDrive/\n",
        "!wget -nc http://images.cocodataset.org/zips/unlabeled2017.zip -P /content/drive/MyDrive/\n",
        "'''\n",
        "\n",
        "'''\n",
        "!wget -nc http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/drive/MyDrive/\n",
        "!wget -nc http://images.cocodataset.org/annotations/image_info_test2017.zip -P /content/drive/MyDrive/\n",
        "!wget -nc http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip -P /content/drive/MyDrive/\n",
        "!wget -nc http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip -P /content/drive/MyDrive/\n",
        "'''\n",
        "\n",
        "'''\n",
        "!wget -nc http://images.cocodataset.org/zips/train2017.zip -P ./data\n",
        "!wget -nc http://images.cocodataset.org/zips/test2017.zip -P ./data\n",
        "!wget -nc http://images.cocodataset.org/zips/val2017.zip -P ./data\n",
        "!wget -nc http://images.cocodataset.org/zips/unlabeled2017.zip -P ./data\n",
        "'''\n",
        "\n",
        "'''\n",
        "!wget -nc http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P ./data\n",
        "!wget -nc http://images.cocodataset.org/annotations/image_info_test2017.zip -P ./data\n",
        "!wget -nc http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip -P ./data\n",
        "!wget -nc http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip -P ./data\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMRMtZAzbkRt"
      },
      "outputs": [],
      "source": [
        "# coco 및 모든 하위 data 삭제\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree(\"/content/coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "MIKCckIRiBKo",
        "outputId": "5d664bb6-a91f-41ea-ae62-9b9751796f3f"
      },
      "outputs": [],
      "source": [
        "# if you have zip files in your drive, and not unzipped yet\n",
        "# if you have unzipped directories of all of these in your drive, SKIP THIS CELL\n",
        "\n",
        "'''\n",
        "!sudo apt install pv\n",
        "n_files = !unzip -l /content/drive/MyDrive/train2017.zip | grep .jpg | wc -l\n",
        "!unzip -o /content/drive/MyDrive/train2017.zip -d coco | pv -l -s {n_files[0]} > /dev/null\n",
        "!unzip -o /content/drive/MyDrive/val2017.zip -d coco | pv -l -s {n_files[0]} > /dev/null\n",
        "# 드라이브에 압축풀음 (초기 1회만 사용) (필요시 주석제거)\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "!unzip -n /content/drive/MyDrive/annotations_trainval2017.zip -d /content/drive/MyDrive/annotations_trainval2017\n",
        "!unzip -n /content/drive/MyDrive/test2017.zip -d /content/drive/MyDrive/test2017\n",
        "!unzip -n /content/drive/MyDrive/image_info_test2017.zip -d /content/drive/MyDrive/image_info_test2017\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIbgnFPoZFs3"
      },
      "outputs": [],
      "source": [
        "# path set\n",
        "trainset_path = \"./content/coco/train2017\"\n",
        "trainAnno_path = \"./content/coco/annotations_trainval2017/annotations/instances_train2017.json\"\n",
        "\n",
        "testset_path = \"./content/coco/test2017\"\n",
        "testInfo_path = \"./content/coco/image_info_test2017/annotations/image_info_test2017.json\"\n",
        "\n",
        "valset_path = \"/content/coco/val2017\"\n",
        "valAnno_path = \"/content/coco/annotations_trainval2017/annotations/instances_val2017.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLfnMk720d2i",
        "outputId": "3fa723f0-016c-4386-d8a7-8153ce1f2e65"
      },
      "outputs": [],
      "source": [
        "\n",
        "coco = COCO(trainAnno_path)\n",
        "categories = coco.loadCats(coco.getCatIds())\n",
        "print(categories, type(categories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBq3rysoMI1r"
      },
      "outputs": [],
      "source": [
        "category_id_to_index = {cat['id']: i for i, cat in enumerate(categories)}\n",
        "coco_instance_category_names = ['__background__'] + [cat['name'] for cat in categories]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwwLS7Y6qfa1"
      },
      "outputs": [],
      "source": [
        "class COCODataset(Dataset):\n",
        "    def __init__(self, root_dir, annotation_file, transform=None, set_name='val'):\n",
        "        self.root_dir = root_dir\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.ids = list(self.coco.imgs.keys())\n",
        "        self.transform = transform\n",
        "        self.set_name = set_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = f\"{self.root_dir}/{img_info['file_name']}\"\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            boxes.append([x, y, x+w, y+h])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([img_id]),\n",
        "            'orig_size': torch.as_tensor([img_info['height'], img_info['width']])\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img, target = self.transform(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def image_aspect_ratio(self, image_index):\n",
        "        image = self.coco.loadImgs(self.ids[image_index])[0]\n",
        "        return float(image['width']) / float(image['height'])\n",
        "\n",
        "    def label_to_coco_label(self, label):\n",
        "        return label  # COCO 데이터셋에서는 이미 COCO 레이블을 사용하므로 변환이 필요 없음\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.coco.getCatIds())\n",
        "\n",
        "    @property\n",
        "    def image_ids(self):\n",
        "        return self.ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-QPiBkxqyIe"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    max_width = 0\n",
        "    max_height = 0\n",
        "\n",
        "    for img, target in batch:\n",
        "        max_width = max(max_width, img.shape[2])\n",
        "        max_height = max(max_height, img.shape[1])\n",
        "\n",
        "    for img, target in batch:\n",
        "        pad_width = max_width - img.shape[2]\n",
        "        pad_height = max_height - img.shape[1]\n",
        "        padded_img = pad(img, (0, pad_width, 0, pad_height), mode='constant', value=0)\n",
        "        images.append(padded_img)\n",
        "\n",
        "        boxes = target['boxes']\n",
        "        if boxes.numel() > 0:\n",
        "            # 높이와 너비가 0인 박스 제거\n",
        "            valid_boxes = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
        "            boxes = boxes[valid_boxes]\n",
        "            target['labels'] = target['labels'][valid_boxes]\n",
        "\n",
        "            if boxes.numel() > 0:\n",
        "                # 박스 좌표 정규화 및 작은 값 추가\n",
        "                boxes[:, [0, 2]] = boxes[:, [0, 2]] * (max_width / img.shape[2])\n",
        "                boxes[:, [1, 3]] = boxes[:, [1, 3]] * (max_height / img.shape[1])\n",
        "                boxes[:, 2:] += 1e-5  # 높이와 너비에 작은 값 추가\n",
        "            else:\n",
        "                # 유효한 박스가 없는 경우 더미 박스 추가\n",
        "                boxes = torch.tensor([[0, 0, 1e-5, 1e-5]], dtype=torch.float32)\n",
        "                target['labels'] = torch.tensor([0], dtype=torch.int64)\n",
        "        else:\n",
        "            # 박스가 없는 경우 더미 박스 추가\n",
        "            boxes = torch.tensor([[0, 0, 1e-5, 1e-5]], dtype=torch.float32)\n",
        "            target['labels'] = torch.tensor([0], dtype=torch.int64)\n",
        "\n",
        "        target['boxes'] = boxes\n",
        "        target['orig_size'] = torch.tensor([img.shape[1], img.shape[2]])\n",
        "        target['size'] = torch.tensor([max_height, max_width])\n",
        "        target['pad_size'] = torch.tensor([max_height, max_width])\n",
        "        targets.append(target)\n",
        "\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2AWIfcCq0FV",
        "outputId": "feec61dd-5f96-45a9-838f-7d9a36888b28"
      },
      "outputs": [],
      "source": [
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        image = T.functional.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "class Normalize:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = T.functional.normalize(image, mean=self.mean, std=self.std)\n",
        "        return image, target\n",
        "\n",
        "# 데이터 변환\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.47004986, 0.44683833, 0.40762289], std=[0.24388725, 0.2390123, 0.24204847])\n",
        "])\n",
        "# COCO mean: [0.47004986 0.44683833 0.40762289]\n",
        "# COCO std: [0.24388725 0.2390123  0.24204847]\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = COCODataset(root_dir=trainset_path,\n",
        "                            annotation_file=trainAnno_path,\n",
        "                            transform=transform)\n",
        "\n",
        "test_dataset = COCODataset(root_dir=testset_path,\n",
        "                           annotation_file=testInfo_path,\n",
        "                           transform=transform)\n",
        "\n",
        "val_dataset = COCODataset(root_dir=valset_path,\n",
        "                          annotation_file=valAnno_path,\n",
        "                          transform=transform)\n",
        "# 1000개의 인덱스를 무작위로 선택\n",
        "indices = random.sample(range(len(train_dataset)), 100)\n",
        "\n",
        "# 선택된 인덱스로 서브셋 생성\n",
        "subset = Subset(train_dataset, indices)\n",
        "\n",
        "\n",
        "# train 데이터로더 생성\n",
        "train_loader = DataLoader(subset,\n",
        "                          batch_size=config['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          num_workers=config['worker'],\n",
        "                          collate_fn=collate_fn)\n",
        "# test 데이터로더\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=config['batch_size'],\n",
        "                         shuffle=False,\n",
        "                         num_workers=config['worker'],\n",
        "                         collate_fn=collate_fn)\n",
        "# valid 데이터로더\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                         batch_size=config['batch_size'],\n",
        "                         shuffle=False,\n",
        "                         num_workers=config['worker'],\n",
        "                         collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U3Om6lkEQ5r"
      },
      "source": [
        "## trainloader 이미지 살펴보기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iOX6PGGvsiyB",
        "outputId": "28df2405-bbe8-4900-8ba0-78b91a487987"
      },
      "outputs": [],
      "source": [
        "def denormalize(tensor, mean, std):\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "def visualize_batch(dataloader, num_images=10):\n",
        "    mean = [0.47004986, 0.44683833, 0.40762289]\n",
        "    std = [0.24388725, 0.2390123, 0.24204847]\n",
        "\n",
        "    plt.figure(figsize=(20, 6 * num_images))\n",
        "    for i in range(num_images):\n",
        "        images, targets = next(iter(dataloader))\n",
        "\n",
        "        idx = random.randint(0, images.shape[0] - 1)\n",
        "        image = images[idx]\n",
        "        target = targets[idx]\n",
        "\n",
        "        image = denormalize(image.clone(), mean, std)\n",
        "        image = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        orig_height, orig_width = target['orig_size']\n",
        "        pad_height, pad_width = target['pad_size']\n",
        "\n",
        "        plt.subplot(num_images, 1, i + 1)\n",
        "        plt.imshow(image[:orig_height, :orig_width])  # 원본 이미지 크기만큼만 표시\n",
        "        plt.title(f\"Image ID: {target['image_id'].item()}\")\n",
        "\n",
        "        for box, label_id in zip(target['boxes'], target['labels']):\n",
        "            # 패딩을 고려하여 바운딩 박스 좌표 조정\n",
        "            x1, y1, x2, y2 = box.numpy()\n",
        "            x1 = x1 * orig_width / pad_width\n",
        "            x2 = x2 * orig_width / pad_width\n",
        "            y1 = y1 * orig_height / pad_height\n",
        "            y2 = y2 * orig_height / pad_height\n",
        "\n",
        "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='r', linewidth=2)\n",
        "            plt.gca().add_patch(rect)\n",
        "\n",
        "            try:\n",
        "                index = category_id_to_index[label_id.item()]\n",
        "                class_name = coco_instance_category_names[index + 1]  # +1 because of background class\n",
        "            except KeyError:\n",
        "                class_name = f\"Unknown ({label_id.item()})\"\n",
        "            plt.text(x1, y1, class_name, bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 데이터 시각화\n",
        "visualize_batch(train_loader, num_images=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrCinkAfDlE-"
      },
      "source": [
        "## 4. 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr20WweNgJ3j",
        "outputId": "b09a495f-8c6c-4f68-a47b-b0525ede3ceb"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bDwwSxdoRkf"
      },
      "outputs": [],
      "source": [
        "def _sum(x: List[Tensor]) -> Tensor:\n",
        "    res = x[0]\n",
        "    for i in x[1:]:\n",
        "        res = res + i\n",
        "    return res\n",
        "\n",
        "\n",
        "# Define Matcher and BoxCoder classes manually\n",
        "class Matcher:\n",
        "    BELOW_LOW_THRESHOLD = -1\n",
        "    BETWEEN_THRESHOLDS = -2\n",
        "\n",
        "    def __init__(self, high_threshold, low_threshold, allow_low_quality_matches=False):\n",
        "        self.high_threshold = high_threshold\n",
        "        self.low_threshold = low_threshold\n",
        "        self.allow_low_quality_matches = allow_low_quality_matches\n",
        "\n",
        "    def __call__(self, match_quality_matrix):\n",
        "        matched_vals, matches = match_quality_matrix.max(dim=0)\n",
        "        if self.allow_low_quality_matches:\n",
        "            all_matches = matches.clone()\n",
        "        else:\n",
        "            all_matches = None\n",
        "\n",
        "        below_low_threshold = matched_vals < self.low_threshold\n",
        "        between_thresholds = (matched_vals >= self.low_threshold) & (matched_vals < self.high_threshold)\n",
        "\n",
        "        matches[below_low_threshold] = self.BELOW_LOW_THRESHOLD\n",
        "        matches[between_thresholds] = self.BETWEEN_THRESHOLDS\n",
        "\n",
        "        if self.allow_low_quality_matches:\n",
        "            self.set_low_quality_matches_(matches, all_matches, match_quality_matrix)\n",
        "\n",
        "        return matches\n",
        "\n",
        "    def set_low_quality_matches_(self, matches, all_matches, match_quality_matrix):\n",
        "        highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)\n",
        "        gt_pred_pairs_of_highest_quality = torch.where(match_quality_matrix == highest_quality_foreach_gt[:, None])\n",
        "        pred_inds_to_update = gt_pred_pairs_of_highest_quality[1]\n",
        "        matches[pred_inds_to_update] = all_matches[pred_inds_to_update]\n",
        "\n",
        "class BoxCoder:\n",
        "    def __init__(self, weights, bbox_xform_clip=math.log(1000. / 16)):\n",
        "        self.weights = weights\n",
        "        self.bbox_xform_clip = bbox_xform_clip\n",
        "\n",
        "    def encode(self, reference_boxes, proposals):\n",
        "        wx, wy, ww, wh = self.weights\n",
        "        targets = []\n",
        "        for boxes_per_image, proposals_per_image in zip(reference_boxes, proposals):\n",
        "            targets_per_image = self.encode_single(boxes_per_image, proposals_per_image)\n",
        "            targets.append(targets_per_image)\n",
        "        return targets\n",
        "\n",
        "    def encode_single(self, reference_boxes, proposals):\n",
        "        dtype = reference_boxes.dtype\n",
        "        device = reference_boxes.device\n",
        "        weights = torch.as_tensor(self.weights, dtype=dtype, device=device)\n",
        "        targets = encode_boxes(reference_boxes, proposals, weights)\n",
        "        return targets\n",
        "\n",
        "    def decode(self, rel_codes, boxes):\n",
        "        assert isinstance(boxes, (list, tuple))\n",
        "        assert isinstance(rel_codes, torch.Tensor)\n",
        "        boxes_per_image = [b.size(0) for b in boxes]\n",
        "        concat_boxes = torch.cat(boxes, dim=0)\n",
        "        box_sum = 0\n",
        "        for val in boxes_per_image:\n",
        "            box_sum += val\n",
        "        pred_boxes = self.decode_single(\n",
        "            rel_codes.reshape(box_sum, -1), concat_boxes\n",
        "        )\n",
        "        return pred_boxes.reshape(box_sum, -1, 4)\n",
        "\n",
        "    def decode_single(self, rel_codes, boxes):\n",
        "        boxes = boxes.to(rel_codes.dtype)\n",
        "        widths = boxes[:, 2] - boxes[:, 0]\n",
        "        heights = boxes[:, 3] - boxes[:, 1]\n",
        "        ctr_x = boxes[:, 0] + 0.5 * widths\n",
        "        ctr_y = boxes[:, 1] + 0.5 * heights\n",
        "\n",
        "        wx, wy, ww, wh = self.weights\n",
        "        dx = rel_codes[:, 0::4] / wx\n",
        "        dy = rel_codes[:, 1::4] / wy\n",
        "        dw = rel_codes[:, 2::4] / ww\n",
        "        dh = rel_codes[:, 3::4] / wh\n",
        "\n",
        "        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n",
        "        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n",
        "\n",
        "        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n",
        "        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n",
        "        pred_w = torch.exp(dw) * widths[:, None]\n",
        "        pred_h = torch.exp(dh) * heights[:, None]\n",
        "\n",
        "        pred_boxes = torch.zeros_like(rel_codes)\n",
        "        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
        "        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
        "        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
        "        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "        return pred_boxes\n",
        "\n",
        "\n",
        "from torchvision.ops.focal_loss import sigmoid_focal_loss\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"RetinaNet\", \"retinanet_resnet50_fpn\"\n",
        "]\n",
        "\n",
        "\n",
        "def _sum(x: List[Tensor]) -> Tensor:\n",
        "    res = x[0]\n",
        "    for i in x[1:]:\n",
        "        res = res + i\n",
        "    return res\n",
        "\n",
        "\n",
        "class RetinaNetHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A regression and classification head for use in RetinaNet.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_anchors (int): number of anchors to be predicted\n",
        "        num_classes (int): number of classes to be predicted\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_anchors, num_classes):\n",
        "        super().__init__()\n",
        "        self.classification_head = RetinaNetClassificationHead(in_channels, num_anchors, num_classes)\n",
        "        self.regression_head = RetinaNetRegressionHead(in_channels, num_anchors)\n",
        "\n",
        "    def compute_loss(self, targets, head_outputs, anchors, matched_idxs):\n",
        "        # type: (List[Dict[str, Tensor]], Dict[str, Tensor], List[Tensor], List[Tensor]) -> Dict[str, Tensor]\n",
        "        return {\n",
        "            'classification': self.classification_head.compute_loss(targets, head_outputs, matched_idxs),\n",
        "            'bbox_regression': self.regression_head.compute_loss(targets, head_outputs, anchors, matched_idxs),\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        # type: (List[Tensor]) -> Dict[str, Tensor]\n",
        "        return {\n",
        "            'cls_logits': self.classification_head(x),\n",
        "            'bbox_regression': self.regression_head(x)\n",
        "        }\n",
        "\n",
        "\n",
        "class RetinaNetClassificationHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A classification head for use in RetinaNet.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_anchors (int): number of anchors to be predicted\n",
        "        num_classes (int): number of classes to be predicted\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_anchors, num_classes, prior_probability=0.01):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = []\n",
        "        for _ in range(4):\n",
        "            conv.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
        "            conv.append(nn.ReLU())\n",
        "        self.conv = nn.Sequential(*conv)\n",
        "\n",
        "        for layer in self.conv.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                torch.nn.init.normal_(layer.weight, std=0.01)\n",
        "                torch.nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "        self.cls_logits = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n",
        "        torch.nn.init.normal_(self.cls_logits.weight, std=0.01)\n",
        "        torch.nn.init.constant_(self.cls_logits.bias, -math.log((1 - prior_probability) / prior_probability))\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_anchors = num_anchors\n",
        "\n",
        "        # This is to fix using det_utils.Matcher.BETWEEN_THRESHOLDS in TorchScript.\n",
        "        # TorchScript doesn't support class attributes.\n",
        "        # https://github.com/pytorch/vision/pull/1697#issuecomment-630255584\n",
        "        self.BETWEEN_THRESHOLDS = Matcher.BETWEEN_THRESHOLDS\n",
        "\n",
        "    def compute_loss(self, targets, head_outputs, matched_idxs):\n",
        "        # type: (List[Dict[str, Tensor]], Dict[str, Tensor], List[Tensor]) -> Tensor\n",
        "        losses = []\n",
        "\n",
        "        cls_logits = head_outputs['cls_logits']\n",
        "\n",
        "        for targets_per_image, cls_logits_per_image, matched_idxs_per_image in zip(targets, cls_logits, matched_idxs):\n",
        "            # determine only the foreground\n",
        "            foreground_idxs_per_image = matched_idxs_per_image >= 0\n",
        "            num_foreground = foreground_idxs_per_image.sum()\n",
        "\n",
        "            # create the target classification\n",
        "            gt_classes_target = torch.zeros_like(cls_logits_per_image)\n",
        "            gt_classes_target[\n",
        "                foreground_idxs_per_image,\n",
        "                targets_per_image['labels'][matched_idxs_per_image[foreground_idxs_per_image]]\n",
        "            ] = 1.0\n",
        "\n",
        "            # find indices for which anchors should be ignored\n",
        "            valid_idxs_per_image = matched_idxs_per_image != self.BETWEEN_THRESHOLDS\n",
        "\n",
        "            # compute the classification loss\n",
        "            losses.append(sigmoid_focal_loss(\n",
        "                cls_logits_per_image[valid_idxs_per_image],\n",
        "                gt_classes_target[valid_idxs_per_image],\n",
        "                reduction='sum',\n",
        "            ) / max(1, num_foreground))\n",
        "\n",
        "        return _sum(losses) / len(targets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        all_cls_logits = []\n",
        "\n",
        "        for features in x:\n",
        "            cls_logits = self.conv(features)\n",
        "            cls_logits = self.cls_logits(cls_logits)\n",
        "\n",
        "            # Permute classification output from (N, A * K, H, W) to (N, HWA, K).\n",
        "            N, _, H, W = cls_logits.shape\n",
        "            cls_logits = cls_logits.view(N, -1, self.num_classes, H, W)\n",
        "            cls_logits = cls_logits.permute(0, 3, 4, 1, 2)\n",
        "            cls_logits = cls_logits.reshape(N, -1, self.num_classes)  # Size=(N, HWA, 4)\n",
        "\n",
        "            all_cls_logits.append(cls_logits)\n",
        "\n",
        "        return torch.cat(all_cls_logits, dim=1)\n",
        "\n",
        "\n",
        "class RetinaNetRegressionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A regression head for use in RetinaNet.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_anchors (int): number of anchors to be predicted\n",
        "    \"\"\"\n",
        "    __annotations__ = {\n",
        "        'box_coder': BoxCoder,\n",
        "    }\n",
        "\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = []\n",
        "        for _ in range(4):\n",
        "            conv.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
        "            conv.append(nn.ReLU())\n",
        "        self.conv = nn.Sequential(*conv)\n",
        "\n",
        "        self.bbox_reg = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n",
        "        torch.nn.init.normal_(self.bbox_reg.weight, std=0.01)\n",
        "        torch.nn.init.zeros_(self.bbox_reg.bias)\n",
        "\n",
        "        for layer in self.conv.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                torch.nn.init.normal_(layer.weight, std=0.01)\n",
        "                torch.nn.init.zeros_(layer.bias)\n",
        "\n",
        "        self.box_coder = BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n",
        "\n",
        "    def compute_loss(self, targets, head_outputs, anchors, matched_idxs):\n",
        "        # type: (List[Dict[str, Tensor]], Dict[str, Tensor], List[Tensor], List[Tensor]) -> Tensor\n",
        "        losses = []\n",
        "\n",
        "        bbox_regression = head_outputs['bbox_regression']\n",
        "\n",
        "        for targets_per_image, bbox_regression_per_image, anchors_per_image, matched_idxs_per_image in \\\n",
        "                zip(targets, bbox_regression, anchors, matched_idxs):\n",
        "            # determine only the foreground indices, ignore the rest\n",
        "            foreground_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]\n",
        "            num_foreground = foreground_idxs_per_image.numel()\n",
        "\n",
        "            # select only the foreground boxes\n",
        "            matched_gt_boxes_per_image = targets_per_image['boxes'][matched_idxs_per_image[foreground_idxs_per_image]]\n",
        "            bbox_regression_per_image = bbox_regression_per_image[foreground_idxs_per_image, :]\n",
        "            anchors_per_image = anchors_per_image[foreground_idxs_per_image, :]\n",
        "\n",
        "            # compute the regression targets\n",
        "            target_regression = self.box_coder.encode_single(matched_gt_boxes_per_image, anchors_per_image)\n",
        "\n",
        "            # compute the loss\n",
        "            losses.append(torch.nn.functional.l1_loss(\n",
        "                bbox_regression_per_image,\n",
        "                target_regression,\n",
        "                reduction='sum'\n",
        "            ) / max(1, num_foreground))\n",
        "\n",
        "        return _sum(losses) / max(1, len(targets))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        all_bbox_regression = []\n",
        "\n",
        "        for features in x:\n",
        "            bbox_regression = self.conv(features)\n",
        "            bbox_regression = self.bbox_reg(bbox_regression)\n",
        "\n",
        "            # Permute bbox regression output from (N, 4 * A, H, W) to (N, HWA, 4).\n",
        "            N, _, H, W = bbox_regression.shape\n",
        "            bbox_regression = bbox_regression.view(N, -1, 4, H, W)\n",
        "            bbox_regression = bbox_regression.permute(0, 3, 4, 1, 2)\n",
        "            bbox_regression = bbox_regression.reshape(N, -1, 4)  # Size=(N, HWA, 4)\n",
        "\n",
        "            all_bbox_regression.append(bbox_regression)\n",
        "\n",
        "        return torch.cat(all_bbox_regression, dim=1)\n",
        "\n",
        "\n",
        "class RetinaNet(nn.Module):\n",
        "\n",
        "    __annotations__ = {\n",
        "        'box_coder': BoxCoder,\n",
        "        'proposal_matcher': Matcher,\n",
        "    }\n",
        "\n",
        "    def __init__(self, backbone, num_classes,\n",
        "                 # transform parameters\n",
        "                 min_size=800, max_size=1333,\n",
        "                 image_mean=None, image_std=None,\n",
        "                 # Anchor parameters\n",
        "                 anchor_generator=None, head=None,\n",
        "                 proposal_matcher=None,\n",
        "                 score_thresh=0.05,\n",
        "                 nms_thresh=0.5,\n",
        "                 detections_per_img=300,\n",
        "                 fg_iou_thresh=0.5, bg_iou_thresh=0.4,\n",
        "                 topk_candidates=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        if not hasattr(backbone, \"out_channels\"):\n",
        "            raise ValueError(\n",
        "                \"backbone should contain an attribute out_channels \"\n",
        "                \"specifying the number of output channels (assumed to be the \"\n",
        "                \"same for all the levels)\")\n",
        "        self.backbone = backbone\n",
        "\n",
        "        assert isinstance(anchor_generator, (AnchorGenerator, type(None)))\n",
        "\n",
        "        if anchor_generator is None:\n",
        "            anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [32, 64, 128, 256, 512])\n",
        "            aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
        "            anchor_generator = AnchorGenerator(\n",
        "                anchor_sizes, aspect_ratios\n",
        "            )\n",
        "        self.anchor_generator = anchor_generator\n",
        "\n",
        "        if head is None:\n",
        "            head = RetinaNetHead(backbone.out_channels, anchor_generator.num_anchors_per_location()[0], num_classes)\n",
        "        self.head = head\n",
        "\n",
        "        if proposal_matcher is None:\n",
        "            proposal_matcher = Matcher(\n",
        "                fg_iou_thresh,\n",
        "                bg_iou_thresh,\n",
        "                allow_low_quality_matches=True,\n",
        "            )\n",
        "        self.proposal_matcher = proposal_matcher\n",
        "\n",
        "        self.box_coder = BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n",
        "\n",
        "        if image_mean is None:\n",
        "            image_mean = [0.47004986, 0.44683833, 0.40762289]\n",
        "        if image_std is None:\n",
        "            image_std = [0.24388725, 0.2390123, 0.24204847]\n",
        "\n",
        "        self.transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
        "\n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.detections_per_img = detections_per_img\n",
        "        self.topk_candidates = topk_candidates\n",
        "\n",
        "        # used only on torchscript mode\n",
        "        self._has_warned = False\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def eager_outputs(self, losses, detections):\n",
        "        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
        "        if self.training:\n",
        "            return losses\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def compute_loss(self, targets, head_outputs, anchors):\n",
        "        # type: (List[Dict[str, Tensor]], Dict[str, Tensor], List[Tensor]) -> Dict[str, Tensor]\n",
        "        matched_idxs = []\n",
        "        for anchors_per_image, targets_per_image in zip(anchors, targets):\n",
        "            if targets_per_image['boxes'].numel() == 0:\n",
        "                matched_idxs.append(torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64))\n",
        "                continue\n",
        "\n",
        "            match_quality_matrix = box_ops.box_iou(targets_per_image['boxes'], anchors_per_image)\n",
        "            matched_idxs.append(self.proposal_matcher(match_quality_matrix))\n",
        "\n",
        "        return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)\n",
        "\n",
        "    def postprocess_detections(self, head_outputs, anchors, image_shapes):\n",
        "        # type: (Dict[str, List[Tensor]], List[List[Tensor]], List[Tuple[int, int]]) -> List[Dict[str, Tensor]]\n",
        "        class_logits = head_outputs['cls_logits']\n",
        "        box_regression = head_outputs['bbox_regression']\n",
        "\n",
        "        num_images = len(image_shapes)\n",
        "\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "\n",
        "        for index in range(num_images):\n",
        "            box_regression_per_image = [br[index] for br in box_regression]\n",
        "            logits_per_image = [cl[index] for cl in class_logits]\n",
        "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
        "\n",
        "            image_boxes = []\n",
        "            image_scores = []\n",
        "            image_labels = []\n",
        "\n",
        "            for box_regression_per_level, logits_per_level, anchors_per_level in \\\n",
        "                    zip(box_regression_per_image, logits_per_image, anchors_per_image):\n",
        "                num_classes = logits_per_level.shape[-1]\n",
        "\n",
        "                # remove low scoring boxes\n",
        "                scores_per_level = torch.sigmoid(logits_per_level).flatten()\n",
        "                keep_idxs = scores_per_level > self.score_thresh\n",
        "                scores_per_level = scores_per_level[keep_idxs]\n",
        "                topk_idxs = torch.where(keep_idxs)[0]\n",
        "\n",
        "                # keep only topk scoring predictions\n",
        "                num_topk = min(self.topk_candidates, topk_idxs.size(0))\n",
        "                scores_per_level, idxs = scores_per_level.topk(num_topk)\n",
        "                topk_idxs = topk_idxs[idxs]\n",
        "\n",
        "                anchor_idxs = topk_idxs // num_classes\n",
        "                labels_per_level = topk_idxs % num_classes\n",
        "\n",
        "                boxes_per_level = self.box_coder.decode_single(box_regression_per_level[anchor_idxs],\n",
        "                                                               anchors_per_level[anchor_idxs])\n",
        "                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n",
        "\n",
        "                image_boxes.append(boxes_per_level)\n",
        "                image_scores.append(scores_per_level)\n",
        "                image_labels.append(labels_per_level)\n",
        "\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)\n",
        "            image_scores = torch.cat(image_scores, dim=0)\n",
        "            image_labels = torch.cat(image_labels, dim=0)\n",
        "\n",
        "            # non-maximum suppression\n",
        "            keep = box_ops.batched_nms(image_boxes, image_scores, image_labels, self.nms_thresh)\n",
        "            keep = keep[:self.detections_per_img]\n",
        "\n",
        "            detections.append({\n",
        "                'boxes': image_boxes[keep],\n",
        "                'scores': image_scores[keep],\n",
        "                'labels': image_labels[keep],\n",
        "            })\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
        "        # Forward pass 프로파일링 시작\n",
        "        with torch.profiler.record_function(\"model_forward_pass\"):\n",
        "          if self.training and targets is None:\n",
        "              raise ValueError(\"In training mode, targets should be passed\")\n",
        "\n",
        "          if self.training:\n",
        "              assert targets is not None\n",
        "              for target in targets:\n",
        "                  boxes = target[\"boxes\"]\n",
        "                  if isinstance(boxes, torch.Tensor):\n",
        "                      if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n",
        "                          raise ValueError(\"Expected target boxes to be a tensor\"\n",
        "                                          \"of shape [N, 4], got {:}.\".format(\n",
        "                                              boxes.shape))\n",
        "                  else:\n",
        "                      raise ValueError(\"Expected target boxes to be of type \"\n",
        "                                      \"Tensor, got {:}.\".format(type(boxes)))\n",
        "\n",
        "          # get the original image sizes\n",
        "          original_image_sizes: List[Tuple[int, int]] = []\n",
        "          for img in images:\n",
        "              val = img.shape[-2:]\n",
        "              assert len(val) == 2\n",
        "              original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "          # transform the input\n",
        "          images, targets = self.transform(images, targets)\n",
        "\n",
        "          # Check for degenerate boxes\n",
        "          # TODO: Move this to a function\n",
        "          if targets is not None:\n",
        "              for target_idx, target in enumerate(targets):\n",
        "                  boxes = target[\"boxes\"]\n",
        "                  degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
        "                  if degenerate_boxes.any():\n",
        "                      # print the first degenerate box\n",
        "                      bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
        "                      degen_bb: List[float] = boxes[bb_idx].tolist()\n",
        "                      raise ValueError(\"All bounding boxes should have positive height and width.\"\n",
        "                                      \" Found invalid box {} for target at index {}.\"\n",
        "                                      .format(degen_bb, target_idx))\n",
        "\n",
        "          # get the features from the backbone\n",
        "          features = self.backbone(images.tensors)\n",
        "          if isinstance(features, torch.Tensor):\n",
        "              features = OrderedDict([('0', features)])\n",
        "\n",
        "          # TODO: Do we want a list or a dict?\n",
        "          features = list(features.values())\n",
        "\n",
        "          # compute the retinanet heads outputs using the features\n",
        "          head_outputs = self.head(features)\n",
        "\n",
        "          # create the set of anchors\n",
        "          anchors = self.anchor_generator(images, features)\n",
        "\n",
        "          losses = {}\n",
        "          detections: List[Dict[str, Tensor]] = []\n",
        "          if self.training:\n",
        "              assert targets is not None\n",
        "\n",
        "              # compute the losses\n",
        "              losses = self.compute_loss(targets, head_outputs, anchors)\n",
        "          else:\n",
        "              # recover level sizes\n",
        "              num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
        "              HW = 0\n",
        "              for v in num_anchors_per_level:\n",
        "                  HW += v\n",
        "              HWA = head_outputs['cls_logits'].size(1)\n",
        "              A = HWA // HW\n",
        "              num_anchors_per_level = [hw * A for hw in num_anchors_per_level]\n",
        "\n",
        "              # split outputs per level\n",
        "              split_head_outputs: Dict[str, List[Tensor]] = {}\n",
        "              for k in head_outputs:\n",
        "                  split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n",
        "              split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n",
        "\n",
        "              # compute the detections\n",
        "              detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n",
        "              detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
        "\n",
        "          if torch.jit.is_scripting():\n",
        "              if not self._has_warned:\n",
        "                  warnings.warn(\"RetinaNet always returns a (Losses, Detections) tuple in scripting\")\n",
        "                  self._has_warned = True\n",
        "              return losses, detections\n",
        "        return self.eager_outputs(losses, detections)\n",
        "\n",
        "import torchvision\n",
        "\n",
        "def retinanet_resnext50_fpn(num_classes=91, pretrained_backbone=True, **kwargs):\n",
        "    # ResNext50 백본 로드\n",
        "    backbone = torchvision.models.resnext50_32x4d(pretrained=pretrained_backbone).features\n",
        "\n",
        "    # FPN을 위한 out_channels 설정\n",
        "    backbone.out_channels = 2048\n",
        "\n",
        "    # AnchorGenerator 설정\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "\n",
        "    # RetinaNet 모델 생성\n",
        "    model = RetinaNet(backbone,\n",
        "                      num_classes=num_classes,\n",
        "                      anchor_generator=anchor_generator,\n",
        "                      **kwargs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SykHBlc9DiVN"
      },
      "source": [
        "## 5. 모델 학습 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuhOaFPon-2K"
      },
      "outputs": [],
      "source": [
        "# (기존코드 주석처리,COCO 데이터셋 준비 섹션으로 이동, 2024-10-13)\n",
        "# !wget http://images.cocodataset.org/zips/val2017.zip -O coco/val2017.zip\n",
        "# !unzip /content/coco/val2017.zip -d coco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L59mrO-_xds",
        "outputId": "7cd0a911-0047-4c1a-f1f4-41bda9192e2e"
      },
      "outputs": [],
      "source": [
        "print(val_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDuET_ais6OQ"
      },
      "outputs": [],
      "source": [
        "from pycocotools.cocoeval import COCOeval\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def evaluate_coco(dataset, model, threshold=0.05, num_samples=1000):\n",
        "    model.eval()\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    results = []\n",
        "    image_ids = []\n",
        "\n",
        "    # 전체 데이터셋에서 무작위로 num_samples개의 인덱스 선택\n",
        "    total_samples = len(dataset)\n",
        "    sample_indices = random.sample(range(total_samples), min(num_samples, total_samples))\n",
        "\n",
        "    for i, index in enumerate(sample_indices):\n",
        "        image, target = dataset[index]\n",
        "        image_id = target['image_id'].item()\n",
        "\n",
        "        # 이미지를 모델 입력 형식에 맞게 변환\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(image)\n",
        "\n",
        "        scores = predictions[0]['scores'].cpu().numpy()\n",
        "        labels = predictions[0]['labels'].cpu().numpy()\n",
        "        boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "\n",
        "        # 점수 임계값 적용\n",
        "        keep = np.where(scores > threshold)[0]\n",
        "        scores = scores[keep]\n",
        "        labels = labels[keep]\n",
        "        boxes = boxes[keep]\n",
        "\n",
        "        # COCO 형식으로 박스 변환 (x1, y1, w, h)\n",
        "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
        "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
        "\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            result = {\n",
        "                'image_id': image_id,\n",
        "                'category_id': int(label),  # COCO 카테고리 ID로 변환이 필요할 수 있음\n",
        "                'bbox': box.tolist(),\n",
        "                'score': float(score)\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        image_ids.append(image_id)\n",
        "\n",
        "        # 진행 상황 출력\n",
        "        print(f'{i+1}/{num_samples}', end='\\r')\n",
        "\n",
        "    # 결과를 JSON 파일로 저장\n",
        "    json.dump(results, open('bbox_results.json', 'w'), indent=4)\n",
        "\n",
        "    # COCO 평가 수행\n",
        "    coco_gt = dataset.coco  # dataset에 COCO 객체가 있다고 가정\n",
        "    coco_dt = coco_gt.loadRes('bbox_results.json')\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    coco_eval.params.imgIds = image_ids\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return coco_eval.stats  # mAP 등의 평가 지표 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "id": "ronY5mhlfOEt",
        "outputId": "d04bf55d-f858-4da5-efff-51c01dd50d1c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import torchvision\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.detection.retinanet import RetinaNet, RetinaNetHead\n",
        "import time\n",
        "import gc\n",
        "\n",
        "def create_retinanet_resnext50_fpn(num_classes):\n",
        "    # Load a pre-trained ResNeXt50 model\n",
        "    backbone = torchvision.models.resnext50_32x4d(weights='ResNeXt50_32X4D_Weights.DEFAULT')\n",
        "\n",
        "    # Remove the last two layers (avgpool and fc)\n",
        "    backbone = torch.nn.Sequential(*list(backbone.children())[:-2])\n",
        "\n",
        "    # Freeze the backbone layers\n",
        "    for param in backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # RetinaNet needs to know the number of output channels in the backbone\n",
        "    backbone.out_channels = 2048\n",
        "\n",
        "    # Create anchor generator\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((32, 64, 128, 256, 512),),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
        "    )\n",
        "\n",
        "    # Create RetinaNet head\n",
        "    head = RetinaNetHead(backbone.out_channels, anchor_generator.num_anchors_per_location()[0], num_classes)\n",
        "\n",
        "    # Create the RetinaNet model\n",
        "    model = RetinaNet(backbone, num_classes, anchor_generator=anchor_generator, head=head)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_retinanet_resnext50_fpn(num_classes=91)\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "    epoch_total_loss = 0\n",
        "    interval_start_time = time.time()\n",
        "\n",
        "    # PyTorch Profiler 설정 (에폭마다 프로파일링)\n",
        "    activities = [ProfilerActivity.CPU]\n",
        "    if torch.cuda.is_available():\n",
        "        activities.append(ProfilerActivity.CUDA)\n",
        "\n",
        "    with profile(activities=activities, profile_memory=True, record_shapes=True) as prof:  # 메모리 사용량과 shape도 기록\n",
        "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "            with record_function(\"batch_processing\"):\n",
        "                images = list(image.to(device) for image in images)\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                # Forward pass\n",
        "                with record_function(\"model_forward\"):\n",
        "                    loss_dict = model(images, targets)\n",
        "                    losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                # Backward pass 프로파일링 (더 상세한 연산 추적)\n",
        "                with record_function(\"model_backward_pass\"):\n",
        "                    optimizer.zero_grad()\n",
        "                    with record_function(\"loss_backward\"):\n",
        "                        losses.backward()\n",
        "                with record_function(\"optimizer_step\"):\n",
        "                    optimizer.step()\n",
        "\n",
        "                total_loss += losses.item()\n",
        "                epoch_total_loss += losses.item()  # 에폭 전체 손실 누적\n",
        "\n",
        "            # print_inter 배치마다 진행 상황 출력\n",
        "            if (batch_idx + 1) % config['print_inter'] == 0:\n",
        "                interval_end_time = time.time()\n",
        "                interval_time = interval_end_time - interval_start_time\n",
        "                avg_loss = total_loss / config['print_inter']\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{config['epochs']}], \"\n",
        "                      f\"Batch: {batch_idx + 1}/{len(train_loader)}, \"\n",
        "                      f\"Avg Loss: {avg_loss:.4f}, \"\n",
        "                      f\"{config['print_inter']}배치 소요시간: {interval_time // 60:.0f}min {interval_time % 60:.2f}sec\")\n",
        "\n",
        "                total_loss = 0\n",
        "                interval_start_time = time.time()\n",
        "\n",
        "            # 프로파일링을 위해 일정 수의 배치 후 중단 (예: 100배치)\n",
        "            if batch_idx == 99:\n",
        "                break\n",
        "\n",
        "    # 프로파일링 결과 출력\n",
        "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "    prof.export_chrome_trace(f\"profiler_trace_epoch_{epoch+1}.json\")\n",
        "\n",
        "    del prof\n",
        "    gc.collect()\n",
        "\n",
        "    # 에폭 완료 시간 계산 및 출력\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch [{epoch+1}/{config['epochs']}], Training Loss: {epoch_total_loss / len(train_loader):.4f}\")\n",
        "    print(f\"Epoch [{epoch+1}/{config['epochs']}] training complete, 소요시간: {epoch_time // 60:.0f}min {epoch_time % 60:.0f}sec\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # 현재 에폭의 모델 저장\n",
        "    torch.save(model.state_dict(), f'retinanet_resnext50_fpn_epoch_{epoch+1}.pth')\n",
        "\n",
        "# Evaluation phase (학습만 시킬땐 주석처리)\n",
        "'''\n",
        "print(f\"Starting evaluation for epoch {epoch+1}\")\n",
        "model.eval()\n",
        "evaluate_coco(val_dataset, model, threshold=0.05)\n",
        "\n",
        "# 최종 모델 저장\n",
        "torch.save(model.state_dict(), 'retinanet_resnext50_fpn_final.pth')\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tDVhjPLs2jb"
      },
      "outputs": [],
      "source": [
        "# 할당된 메모리 해제\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
